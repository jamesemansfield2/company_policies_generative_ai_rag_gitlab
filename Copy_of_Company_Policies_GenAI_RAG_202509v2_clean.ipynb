{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesemansfield2/company_policies_generative_ai_rag_gitlab/blob/main/Copy_of_Company_Policies_GenAI_RAG_202509v2_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48228430"
      },
      "source": [
        "# Company Policy Assistant, Generative AI RAG\n",
        "\n",
        "This notebook demonstrates building a policy assistant to help employees efficiently search for company policies. The goal is to provide quick and relevant answers to employee questions based on indexed company policies.\n",
        "\n",
        "The assistant is designed to act as a strict company policy assistant for GitLab, adhering to the following rules:\n",
        "- Answer ONLY from the provided context.\n",
        "- Quote exact lines where possible.\n",
        "- Include policy title and effective date.\n",
        "- If the information is not found, state \"Not found in the indexed policies\" and suggest who to ask.\n",
        "- Keep answers concise, using bullets when useful.\n",
        "\n",
        "Sample questions used to test the assistant include:\n",
        "- \"What is the acceptable use policy for company systems?\"\n",
        "- \"How are conflicts of interest handled?\"\n",
        "- \"What is the code of conduct reporting path?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c67adde"
      },
      "source": [
        "This cell sets the `GEMINI_API_KEY` environment variable, which is required to authenticate with the Gemini API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS-0ULKvq6AG",
        "outputId": "e4db5158-bf9b-46b4-b183-c1091631e621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: GEMINI_API_KEY=# [INSERT GOOGLE GEMINI API KEY]\n"
          ]
        }
      ],
      "source": [
        "%env GEMINI_API_KEY = # [INSERT GOOGLE GEMINI API KEY]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10ecbccb"
      },
      "source": [
        "This cell installs the necessary libraries and packages for the policy assistant, including `torch`, `sentence-transformers`, `faiss-cpu`, `rank-bm25`, and `google-genai`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf7cvleR3KkQ",
        "outputId": "c50f0eb5-d407-481b-ea9a-403e51660378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Cannot install faiss-cpu==1.8.0.post1, numpy==2.0.2, pandas==2.2.2, scikit-learn==1.6.1 and scipy==1.15.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Install done. Now: Runtime → Restart runtime (once), then run the next cell.\n"
          ]
        }
      ],
      "source": [
        "# ==== Clean install (Colab / Py3.12) — manual restart ====\n",
        "\n",
        "# Quiet the pydevd \"frozen modules\" warning (harmless, but noisy)\n",
        "import os\n",
        "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
        "\n",
        "# Keep pip tooling stable; pin setuptools <81 to avoid pkg_resources deprecation spam\n",
        "!pip -q install -U pip \"setuptools<81\" wheel\n",
        "\n",
        "# Pin a Colab-friendly, NumPy-2-safe stack that avoids resolver thrash\n",
        "!pip -q install --upgrade --force-reinstall --no-cache-dir --prefer-binary \\\n",
        "  \"numpy==2.0.2\" \\\n",
        "  \"scipy==1.15.1\" \\\n",
        "  \"pandas==2.2.2\" \\\n",
        "  \"requests==2.32.3\" \\\n",
        "  \"google-auth==2.38.0\" \\\n",
        "  \"packaging==24.2\" \\\n",
        "  \"cryptography==43.0.3\" \\\n",
        "  \"fsspec==2025.3.2\" \\\n",
        "  \"scikit-learn==1.6.1\" \\\n",
        "  \"jedi>=0.19.1\" \\\n",
        "  \"faiss-cpu==1.8.0.post1\" \\\n",
        "  \"torch==2.4.1\" \"torchvision==0.19.1\" \"torchaudio==2.4.1\" \\\n",
        "  \"sentence-transformers>=2.7.0\" \\\n",
        "  \"rank-bm25==0.2.2\" \\\n",
        "  \"beautifulsoup4>=4.12.3\" \"lxml>=5.2.2\" \"pdfminer.six>=20231228\" \\\n",
        "  \"python-docx>=1.1.2\" \"tldextract>=5.1.2\" \"google-genai\"\n",
        "\n",
        "print(\"✅ Install done. Now: Runtime → Restart runtime (once), then run the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtY8pD4K3N59",
        "outputId": "2cb86fca-1183-4673-bf4b-d8a47a06c14c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numpy 2.0.2 | pandas 2.2.2 | scipy 1.15.1 | torch 2.4.1+cu121\n",
            "faiss 1.12.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, pandas as pd, scipy, torch\n",
        "print(\"numpy\", np.__version__, \"| pandas\", pd.__version__, \"| scipy\", scipy.__version__, \"| torch\", torch.__version__)\n",
        "import faiss\n",
        "print(\"faiss\", getattr(faiss, \"__version__\", \"n/a\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bf81d97"
      },
      "source": [
        "This cell imports the required libraries and sets up the Gemini API client. It also defines the `MODEL_ID` to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoGJdsuwqtFk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Put your key here or via Colab \"Secrets\":  Settings ▶️ Variables ▶️ GEMINI_API_KEY\n",
        "os.environ.setdefault(\"GEMINI_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "\n",
        "# Developer API (AI Studio) client. For Vertex AI, see the commented block below.\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "# --- If you prefer Vertex AI instead of the Developer API, use this:\n",
        "# os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"true\"\n",
        "# os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-project-id\"\n",
        "# os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\"\n",
        "# client = genai.Client()  # uses env above\n",
        "# MODEL_ID = \"gemini-2.5-flash\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5c5b215"
      },
      "source": [
        "This cell defines the crawling process to collect policy documents from specified URLs within allowed domains and paths. It saves the raw HTML content and returns a list of crawled page URLs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h2YjAgW-rIM"
      },
      "source": [
        "The RAG (Retrieval Augmented Generation) functionality is implemented across several steps and functions.\n",
        "\n",
        "The key parts of the RAG process are:\n",
        "\n",
        "Data Loading and Indexing: The code in the cell with ID B5F9xCXArVsN handles processing the crawled documents, extracting text, splitting it into chunks, and building the searchable index using FAISS (for vector similarity) and BM25 (for keyword matching). This sets up the \"Retrieval\" part of RAG.\n",
        "\n",
        "Hybrid Search: The hybrid_search function within the cell with ID bJwHTfHkrXWW combines the results of the vector search and BM25 search to retrieve the most relevant document chunks based on the user's query. This is a crucial part of the \"Retrieval\" step.\n",
        "\n",
        "Context Creation and Answer Generation: The _context and gemini_generate functions, also in the cell with ID bJwHTfHkrXWW, take the retrieved chunks, format them as context, and then use the Gemini model to generate an answer based on that context. This is the \"Augmented Generation\" part of RAG.\n",
        "\n",
        "So, while there isn't one single \"RAG function,\" the process is distributed across these functions and cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URcsFQQmrC4i",
        "outputId": "f36d2569-cbde-4a3c-89e9-550ea3452482"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os, re, tldextract, requests\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import deque\n",
        "\n",
        "DATA_DIR = \"/content/policy_data\"\n",
        "RAW_DIR = f\"{DATA_DIR}/raw\"\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "\n",
        "COMPANY = \"GitLab\"\n",
        "ALLOW_DOMAINS = {\"about.gitlab.com\"}\n",
        "PATH_ALLOW_PREFIXES = (\"/handbook\", \"/support\", \"/community\", \"/company/policies\")\n",
        "SEED_URLS = [\n",
        "    \"https://about.gitlab.com/handbook/\",\n",
        "    \"https://about.gitlab.com/support/general-policies/\",\n",
        "    \"https://about.gitlab.com/community/contribute/code-of-conduct/\",\n",
        "]\n",
        "MAX_PAGES = 200\n",
        "TIMEOUT = 15\n",
        "HEADERS = {\"User-Agent\": \"policy-rag-colab/1.0\"}\n",
        "\n",
        "def sha1(s: str) -> str:\n",
        "    import hashlib\n",
        "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n",
        "\n",
        "def allowed(url:str)->bool:\n",
        "    p = urlparse(url)\n",
        "    if p.scheme not in (\"http\", \"https\"):\n",
        "        return False\n",
        "    d = tldextract.extract(url)\n",
        "    domain = \".\".join(part for part in [d.domain, d.suffix] if part)\n",
        "    host = f\"{(d.subdomain+'.') if d.subdomain else ''}{domain}\"\n",
        "    if host not in ALLOW_DOMAINS:\n",
        "        return False\n",
        "    return any(urlparse(url).path.startswith(pref) for pref in PATH_ALLOW_PREFIXES)\n",
        "\n",
        "def canonicalize(url:str, base:str)->str:\n",
        "    url = urljoin(base, url)\n",
        "    p = urlparse(url)\n",
        "    return f\"{p.scheme}://{p.netloc}{p.path}\"  # strip query/fragment\n",
        "\n",
        "def crawl(seed_urls, max_pages=200):\n",
        "    seen, pages = set(), []\n",
        "    q = deque(seed_urls)\n",
        "    while q and len(pages) < max_pages:\n",
        "        url = canonicalize(q.popleft(), \"\")\n",
        "        if url in seen or not allowed(url):\n",
        "            continue\n",
        "        seen.add(url)\n",
        "        try:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
        "            if r.status_code != 200 or \"text/html\" not in r.headers.get(\"Content-Type\",\"\"):\n",
        "                continue\n",
        "            html = r.text\n",
        "            out = os.path.join(RAW_DIR, f\"{sha1(url)}.html\")\n",
        "            with open(out, \"w\", encoding=\"utf-8\") as f: f.write(html)\n",
        "            pages.append(url)\n",
        "            soup = BeautifulSoup(html, \"lxml\")\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                nxt = canonicalize(a[\"href\"], url)\n",
        "                if nxt not in seen and allowed(nxt):\n",
        "                    q.append(nxt)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return pages\n",
        "\n",
        "pages = crawl(SEED_URLS, MAX_PAGES)\n",
        "len(pages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef327e11"
      },
      "source": [
        "This cell processes the raw HTML files, extracts text content, splits the content into chunks, and builds a searchable index using FAISS for vector similarity search and BM25 for keyword matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "59820ac266764fe08d710347a7e9fa83",
            "d2797a0f442d404c996d7d74e839f1f8",
            "43ae1ba660684b13bd6f34286095a58a",
            "4f5b49233b29461d847332ebfa99f148",
            "acfed87659f8449191074bfedadaa714",
            "ff4f7da9b08e46648ca8786009d8334d",
            "3662092e253a443bbb3a0f7e491a0b5b",
            "3465177641ce49338f93f1d3f715c174",
            "1e16fc9a3afd4aec8be75afb4671aa16",
            "b7739aa740884f428507f693a4113cd9",
            "397238f91f0a417e9b4633c79e34490b"
          ]
        },
        "id": "B5F9xCXArVsN",
        "outputId": "1e4062c4-dcd6-4b84-9085-2087c0727ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding 667 chunks…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Index built.\n"
          ]
        }
      ],
      "source": [
        "import os, re, pickle, json, numpy as np, pandas as pd, faiss\n",
        "from bs4 import BeautifulSoup\n",
        "from pdfminer.high_level import extract_text as pdf_extract_text\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "INDEX_DIR = f\"{DATA_DIR}/index\"\n",
        "os.makedirs(INDEX_DIR, exist_ok=True)\n",
        "\n",
        "def read_file_text(path: str) -> str:\n",
        "    ext = path.lower().split('.')[-1]\n",
        "    if ext in (\"txt\",\"md\"):\n",
        "        return open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "    if ext in (\"html\",\"htm\"):\n",
        "        soup = BeautifulSoup(open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read(), \"lxml\")\n",
        "        for tag in soup([\"script\",\"style\",\"noscript\"]): tag.decompose()\n",
        "        for i in range(1,7):\n",
        "            for h in soup.find_all(f\"h{i}\"):\n",
        "                h.insert_before(f\"\\n{'#'*i} {h.get_text(strip=True)}\\n\")\n",
        "        return soup.get_text(\"\\n\", strip=True)\n",
        "    if ext == \"pdf\":\n",
        "        return pdf_extract_text(path) or \"\"\n",
        "    if ext == \"docx\":\n",
        "        import docx\n",
        "        d = docx.Document(path)\n",
        "        return \"\\n\".join(p.text for p in d.paragraphs)\n",
        "    raise ValueError(f\"Unsupported file: {path}\")\n",
        "\n",
        "_heading_re = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n",
        "def _split_by_words(block, target=350, overlap=40):\n",
        "    w = block.split()\n",
        "    if len(w) <= target + 100: return [block.strip()]\n",
        "    out, step = [], target - overlap\n",
        "    for i in range(0, len(w), step):\n",
        "        piece = \" \".join(w[i:i+target]).strip()\n",
        "        if piece: out.append(piece)\n",
        "        if i + target >= len(w): break\n",
        "    return out\n",
        "\n",
        "def split_by_headings(text, target=350, overlap=40):\n",
        "    text = re.sub(r'\\r', '', text)\n",
        "    hs = list(_heading_re.finditer(text))\n",
        "    parts = []\n",
        "    if hs:\n",
        "        for i, m in enumerate(hs):\n",
        "            s, e = m.start(), hs[i+1].start() if i+1<len(hs) else len(text)\n",
        "            parts.extend(_split_by_words(text[s:e].strip(), target, overlap))\n",
        "    else:\n",
        "        parts = _split_by_words(text, target, overlap)\n",
        "    return [c for c in (p.strip() for p in parts) if len(c.split())>25]\n",
        "\n",
        "def discover_files(root):\n",
        "    exts = (\".pdf\",\".docx\",\".html\",\".htm\",\".md\",\".txt\")\n",
        "    acc = []\n",
        "    for r,_,fs in os.walk(root):\n",
        "        for f in fs:\n",
        "            if f.lower().endswith(exts): acc.append(os.path.join(r,f))\n",
        "    return acc\n",
        "\n",
        "paths = discover_files(RAW_DIR)\n",
        "rows, chunks = [], []\n",
        "def sha1(s: str):\n",
        "    import hashlib\n",
        "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n",
        "\n",
        "for path in paths:\n",
        "    try:\n",
        "        raw = read_file_text(path)\n",
        "    except Exception:\n",
        "        continue\n",
        "    title = re.sub(r'[_-]+',' ', os.path.splitext(os.path.basename(path))[0]).title()\n",
        "    doc_id = sha1(path)\n",
        "    eff = None\n",
        "    for rx in [re.compile(r\"Effective Date[:\\s]+([A-Za-z]{3,9}\\s+\\d{1,2},\\s*\\d{4})\", re.I),\n",
        "               re.compile(r\"Last Updated[:\\s]+([A-Za-z]{3,9}\\s+\\d{1,2},\\s*\\d{4})\", re.I)]:\n",
        "        m = rx.search(raw)\n",
        "        if m: eff = m.group(1); break\n",
        "    parts = split_by_headings(raw)\n",
        "    for j, part in enumerate(parts):\n",
        "        chunks.append({\n",
        "            \"chunk_id\": f\"{doc_id}-{j:04d}\",\n",
        "            \"doc_id\": doc_id,\n",
        "            \"title\": title,\n",
        "            \"path\": path,\n",
        "            \"effective_date\": eff,\n",
        "            \"text\": part\n",
        "        })\n",
        "    rows.append({\"doc_id\": doc_id, \"title\": title, \"path\": path, \"effective_date\": eff, \"n_chunks\": len(parts)})\n",
        "\n",
        "pd.DataFrame(rows).to_csv(f\"{INDEX_DIR}/catalog.csv\", index=False)\n",
        "with open(f\"{INDEX_DIR}/chunks.pkl\",\"wb\") as f: pickle.dump({c[\"chunk_id\"]: c for c in chunks}, f)\n",
        "\n",
        "# Embeddings + FAISS\n",
        "texts = [c[\"text\"] for c in chunks]\n",
        "print(f\"Embedding {len(texts)} chunks…\")\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vecs = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
        "vecs = np.array(vecs, dtype=\"float32\")\n",
        "fa = faiss.IndexFlatIP(vecs.shape[1]); fa.add(vecs)\n",
        "faiss.write_index(fa, f\"{INDEX_DIR}/faiss.index\")\n",
        "\n",
        "# BM25\n",
        "tok = [t.lower().split() for t in texts]\n",
        "bm25 = BM25Okapi(tok)\n",
        "with open(f\"{INDEX_DIR}/bm25.pkl\",\"wb\") as f: pickle.dump({\"bm25\": bm25, \"tokenized\": tok}, f)\n",
        "with open(f\"{INDEX_DIR}/positions.json\",\"w\") as f: json.dump({\"chunk_ids\":[c[\"chunk_id\"] for c in chunks]}, f)\n",
        "\n",
        "print(\"✅ Index built.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfed8146"
      },
      "source": [
        "This cell contains the core functions for the policy assistant:\n",
        "- `load_index`: Loads the created index components.\n",
        "- `embed_query`: Embeds the user's query into a vector.\n",
        "- `hybrid_search`: Performs a combined vector and keyword search to retrieve relevant policy chunks.\n",
        "- `_cite`: Formats citations for the retrieved policy chunks.\n",
        "- `_context`: Creates a combined context from the retrieved chunks for the language model.\n",
        "- `gemini_generate`: Calls the Gemini API to generate an answer based on the provided context and system prompt, now including the `top_p` parameter.\n",
        "- `answer`: Orchestrates the search, context creation, and answer generation process, also including the `top_p` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJwHTfHkrXWW"
      },
      "outputs": [],
      "source": [
        "import json, numpy as np, pickle, faiss\n",
        "from google.genai import types # Import types for top_p\n",
        "\n",
        "def load_index():\n",
        "    with open(f\"{INDEX_DIR}/chunks.pkl\",\"rb\") as f: chunk_map = pickle.load(f)\n",
        "    with open(f\"{INDEX_DIR}/bm25.pkl\",\"rb\") as f: bm25 = pickle.load(f)[\"bm25\"]\n",
        "    with open(f\"{INDEX_DIR}/positions.json\",\"r\") as f: chunk_ids = json.load(f)[\"chunk_ids\"]\n",
        "    fa = faiss.read_index(f\"{INDEX_DIR}/faiss.index\")\n",
        "    return chunk_map, bm25, chunk_ids, fa\n",
        "\n",
        "def embed_query(q: str) -> np.ndarray:\n",
        "    m = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    v = m.encode([q], normalize_embeddings=True)\n",
        "    return np.array(v, dtype=\"float32\")\n",
        "\n",
        "def hybrid_search(question: str, top_k: int = 8):\n",
        "    chunk_map, bm25, chunk_ids, fa = load_index()\n",
        "    qv = embed_query(question)\n",
        "    D, I = fa.search(qv, min(50, len(chunk_ids)))\n",
        "    vec_scores, vec_idx = D[0], I[0]\n",
        "    bm25_scores = bm25.get_scores(question.lower().split())\n",
        "\n",
        "    def norm(x):\n",
        "        x = np.array(x);\n",
        "        if x.size==0: return x\n",
        "        mn, mx = x.min(), x.max()\n",
        "        return np.zeros_like(x) if mx-mn<1e-9 else (x-mn)/(mx-mn)\n",
        "\n",
        "    n_bm25 = norm(bm25_scores)\n",
        "    n_vec = np.zeros_like(n_bm25); n_vec[vec_idx] = norm(vec_scores)\n",
        "    fused = 0.55*n_vec + 0.45*n_bm25\n",
        "    top_idx = np.argsort(-fused)[:top_k]\n",
        "    return [(chunk_ids[i], float(fused[i])) for i in top_idx]\n",
        "\n",
        "def _cite(meta):\n",
        "    bits = [meta.get(\"title\") or \"Untitled\"]\n",
        "    if meta.get(\"effective_date\"): bits.append(f\"(Effective: {meta['effective_date']})\")\n",
        "    bits.append(f\"[{os.path.basename(meta.get('path',''))}]\")\n",
        "    return \" \".join(bits)\n",
        "\n",
        "def _context(chunks, max_chars=6000):\n",
        "    ctx, used = [], 0\n",
        "    for c in chunks:\n",
        "        header = f\"### {_cite(c)}\\n\"\n",
        "        piece = header + c[\"text\"].strip() + \"\\n\\n\"\n",
        "        if used + len(piece) > max_chars and ctx: break\n",
        "        ctx.append(piece); used += len(piece)\n",
        "    return \"\".join(ctx)\n",
        "\n",
        "def gemini_generate(system_prompt: str, user_prompt: str, temperature: float = 0.2, top_p: float = 0.95) -> str: # Added top_p\n",
        "    cfg = types.GenerateContentConfig(temperature=temperature, top_p=top_p) # Added top_p to config\n",
        "    # Prefer system instructions via config where available\n",
        "    resp = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=[user_prompt],\n",
        "        config=cfg.with_updates(system_instruction=system_prompt)\n",
        "        if hasattr(cfg, \"with_updates\") else types.GenerateContentConfig(\n",
        "            temperature=temperature,  # fallback; older SDKs may ignore system_instruction\n",
        "            top_p=top_p # Added top_p to fallback config\n",
        "        ),\n",
        "    )\n",
        "    # Robust text extraction across SDK responses\n",
        "    if getattr(resp, \"text\", None):\n",
        "        return resp.text\n",
        "    try:\n",
        "        return \"\".join(\n",
        "            part.text for cand in resp.candidates for part in cand.content.parts if getattr(part, \"text\", None)\n",
        "        )\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def answer(question: str, top_k: int = 6, temperature: float = 0.2, top_p: float = 0.95): # Added temperature and top_p\n",
        "    chunk_map, _, _, _ = load_index()\n",
        "    hits = hybrid_search(question, top_k=top_k)\n",
        "    ranked = [chunk_map[cid] for cid,_ in hits]\n",
        "    context = _context(ranked)\n",
        "\n",
        "    sys_prompt = (\n",
        "        \"You are a strict company-policy assistant for GitLab. \"\n",
        "        \"Rules: Answer ONLY from the provided context; quote exact lines where possible; \"\n",
        "        \"include policy title and effective date; if not found, say 'Not found in the indexed policies' \"\n",
        "        \"and suggest who to ask. Keep answers concise with bullets when useful.\"\n",
        "    )\n",
        "    user_prompt = f\"Question: {question}\\n\\nContext:\\n{context}\"\n",
        "    out = gemini_generate(sys_prompt, user_prompt, temperature=temperature, top_p=top_p) or \"No answer.\" # Passed temperature and top_p\n",
        "    cites = [_cite(c) for c in ranked[:min(5, len(ranked))]]\n",
        "    return {\"answer\": out, \"citations\": cites}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e306c1c2"
      },
      "source": [
        "This cell demonstrates how to use the `answer` function with sample questions and prints the generated answers and citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBjLWyLNrdUX",
        "outputId": "84b41794-5e05-49df-92b8-233fc8f9ef0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: What is the acceptable use policy for company systems?\n",
            "Based on the provided documents, there is no information about the acceptable use policy for company systems. The documents cover topics such as spending company money, transparency, support, legal and corporate affairs departments, and corrective actions.\n",
            "Citations:\n",
            " - 59Ded5F81A63 [59ded5f81a63.html]\n",
            " - 59Ded5F81A63 [59ded5f81a63.html]\n",
            " - A5B26Fb746Ed [a5b26fb746ed.html]\n",
            " - 2D6Ff0E6A0D1 [2d6ff0e6a0d1.html]\n",
            " - 018A581E1Ad0 [018a581e1ad0.html]\n",
            "\n",
            "Q: How are conflicts of interest handled?\n",
            "The provided text does not explicitly detail a process for handling conflicts of interest.\n",
            "\n",
            "However, it does state a principle related to avoiding actions that can arise from conflicts of interest:\n",
            "*   \"Don’t show favoritism as it breeds resentment, destroys employee morale, and creates disincentives for good performance. Seek out ways to be fair to everyone.\"\n",
            "\n",
            "This implies that actions resulting from conflicts of interest (like favoritism) are to be avoided, and fairness should be prioritized. The text does not describe a specific procedure for disclosing, managing, or resolving conflicts of interest when they arise.\n",
            "Citations:\n",
            " - 59Ded5F81A63 [59ded5f81a63.html]\n",
            " - 59Ded5F81A63 [59ded5f81a63.html]\n",
            " - 59Ded5F81A63 [59ded5f81a63.html]\n",
            " - 59Ded5F81A63 [59ded5f81a63.html]\n",
            " - C2531D529376 [c2531d529376.html]\n",
            "\n",
            "Q: What is the code of conduct reporting path?\n",
            "The code of conduct reporting path is as follows:\n",
            "\n",
            "1.  **Contact the GitLab Trust and Safety team** at [email protected]\n",
            "2.  **Utilize the variety of methods for reporting abuse** available within the GitLab product.\n",
            "Citations:\n",
            " - C65D1B007743 [c65d1b007743.html]\n",
            " - C65D1B007743 [c65d1b007743.html]\n",
            " - D31Dd700Dca6 [d31dd700dca6.html]\n",
            " - C65D1B007743 [c65d1b007743.html]\n",
            " - C65D1B007743 [c65d1b007743.html]\n"
          ]
        }
      ],
      "source": [
        "for q in [\n",
        "    \"What is the acceptable use policy for company systems?\",\n",
        "    \"How are conflicts of interest handled?\",\n",
        "    \"What is the code of conduct reporting path?\"\n",
        "]:\n",
        "    res = answer(q)\n",
        "    print(\"\\nQ:\", q)\n",
        "    print(res[\"answer\"])\n",
        "    print(\"Citations:\")\n",
        "    for c in res[\"citations\"]:\n",
        "        print(\" -\", c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joH4XxJq9BFh"
      },
      "source": [
        "Save to github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea1E42gN9AdQ",
        "outputId": "44490825-2118-4b3e-9f97-ccf242def3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Could not update in-memory notebook: Unexpected notebook structure from Colab frontend.\n",
            "Mounted at /content/drive\n",
            "❌ Fallback also failed: Notebook could not be converted from version 1 to version 2 because it's missing a key: cells\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- Colab → GitHub preview fixer (robust) ---\n",
        "# Use this cell just before: File → Save a copy to GitHub…\n",
        "\n",
        "def _clean_notebook_dict(nb: dict) -> dict:\n",
        "    # Remove broken ipywidgets metadata that GitHub can't render\n",
        "    md = nb.get(\"metadata\", {})\n",
        "    md.pop(\"widgets\", None)\n",
        "    for cell in nb.get(\"cells\", []):\n",
        "        # cell-level widget metadata\n",
        "        if isinstance(cell.get(\"metadata\"), dict):\n",
        "            cell[\"metadata\"].pop(\"widgets\", None)\n",
        "        # outputs: drop widget MIME bundles that GitHub can't render\n",
        "        outs = cell.get(\"outputs\", [])\n",
        "        if isinstance(outs, list):\n",
        "            for out in outs:\n",
        "                data = out.get(\"data\")\n",
        "                if isinstance(data, dict):\n",
        "                    data.pop(\"application/vnd.jupyter.widget-view+json\", None)\n",
        "                    data.pop(\"application/vnd.jupyter.widget-state+json\", None)\n",
        "    return nb\n",
        "\n",
        "def clean_current_notebook_for_github(drive_fallback=True):\n",
        "    # 1) Try to clean the live (in-memory) notebook so \"Save a copy to GitHub\" uploads the fixed JSON.\n",
        "    try:\n",
        "        from google.colab import _message\n",
        "        nb = _message.blocking_request(\"get_ipynb\", {})\n",
        "        if not isinstance(nb, dict) or \"cells\" not in nb:\n",
        "            raise RuntimeError(\"Unexpected notebook structure from Colab frontend.\")\n",
        "        nb = _clean_notebook_dict(nb)\n",
        "        # Write cleaned JSON back to the frontend\n",
        "        _message.blocking_request(\"set_ipynb\", {\"ipynb\": nb})\n",
        "        print(\"✅ Cleaned in memory. Now use: File → Save a copy to GitHub…\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not update in-memory notebook: {e}\")\n",
        "\n",
        "    # 2) Fallback: save a cleaned copy to Drive (open it, then Save to GitHub)\n",
        "    if drive_fallback:\n",
        "        try:\n",
        "            from google.colab import _message, drive\n",
        "            import nbformat as nbf\n",
        "            drive.mount(\"/content/drive\", force_remount=True)\n",
        "            nb = _message.blocking_request(\"get_ipynb\", {})\n",
        "            nb = _clean_notebook_dict(nb)\n",
        "            out_path = \"/content/drive/MyDrive/Colab Notebooks/clean_for_github.ipynb\"\n",
        "            nbf.write(nbf.from_dict(nb), out_path, version=4)\n",
        "            print(\"✅ Saved cleaned copy to Drive:\", out_path)\n",
        "            print(\"Next: File → Open notebook → Google Drive → open 'clean_for_github.ipynb',\")\n",
        "            print(\"then File → Save a copy to GitHub…\")\n",
        "            return True\n",
        "        except Exception as e2:\n",
        "            print(f\"❌ Fallback also failed: {e2}\")\n",
        "    return False\n",
        "\n",
        "# Run it:\n",
        "clean_current_notebook_for_github()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}